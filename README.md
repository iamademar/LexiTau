# LexiTau â€” Text-to-SQL with Safety, Explainability, and Multi-Tenancy

LexiTau turns natural-language questions about tenant data into **read-only SQL** with clear explanations. It combines schema profiling, metadata-aware linking, and a policy layer that blocks unsafe statements before execution.

> Where does LexiTau comes from? Lexi = lexical, tau means number in Te Reo MÄori.

ğŸ¥ **Project overview & demo:**

<p align="center">
  <a href="https://www.youtube.com/watch?v=Mhi9dbzRowg">
    <img src="https://img.youtube.com/vi/Mhi9dbzRowg/hqdefault.jpg" alt="Demo video">
  </a>
</p>

---

## What this project demonstrates âœ…

* ğŸ§± **T2SQL pipeline design**: Profiling â†’ metadata â†’ schema linking â†’ SQL draft â†’ safety checks â†’ execution â†’ narrative explanation.
* ğŸ§¾ **Document/OCR ingestion**: Invoice/receipt parsing, line-items, confidence scores, normalization into tenant-scoped tables.
* ğŸ¢ **Multi-tenant isolation**: Per-tenant auth + silos, per-tenant embeddings for semantic search.
* ğŸ›¡ï¸ **Safety/guardrails**: Read-only SQL; table/column allowlists; optional row-filters; parameterized literals; clear, actionable errors.
* ğŸ” **Explainability**: â€œShow SQLâ€ + â€œWhy this answer?â€ lineage and concise tables/charts.
* ğŸ§ª **Testing approach**: Pytest (unit/integration) + Playwright (UX flows); explicit test inventory for auth, OCR adapters, and SQL safety.

---

## Core algorithms & components ğŸ§©

### tl;dr âœ¨

Profile the DB â†’ link the question to the right columns â†’ draft SQL with self-checks â†’ pass strict safety checks â†’ return results **with** a clear explanation.

**1) Profiling â†’ Metadata (â€œlearn the database firstâ€) ğŸ§­**
Before answering questions, the system scans your tables to understand them:

* ğŸ”¢ Counts distinct values, detects common formats (dates, currencies), and typical ranges.
* ğŸ—’ï¸ Writes short, plain-English summaries for each column (a mini data dictionary).
* ğŸ§® Saves embeddings of those summaries so the planner can â€œfindâ€ the right columns later.

*What it looks like:*
â€œ`invoices.total_amount` â€” currency, typically 5â€“5000; `paid` is boolean; `created_at` is a date.â€
These summaries guide the model away from guesswork.

---

**2) Schema linking â†’ SQL draft ğŸ”—â¡ï¸ğŸ§¾**
Given a natural-language question, the system figures out *which* tables/columns are relevant and drafts a SQL query:

* ğŸ§  Maps phrases like â€œunpaid invoices last quarterâ€ â†’ `invoices.paid = false`, date filter on `created_at`, group by customer.
* âœ… **Self-checks** joins/filters/literals against real data (e.g., does `paid` exist? is the date column actually a date?).

*What it looks like:*
Question â†’ *â€œWhich customers have unpaid invoices from Q2?â€*
Draft SQL (simplified):

```sql
SELECT c.name, COUNT(*) AS invoice_count, SUM(i.total_amount) AS amount_due
FROM invoices i
JOIN customers c ON c.id = i.customer_id
WHERE i.paid = FALSE AND i.created_at BETWEEN '2025-04-01' AND '2025-06-30'
GROUP BY c.name
ORDER BY amount_due DESC;
```

If a column or join is wrong, the self-check catches it before execution. âœ…

---

**3) Safety layer (policy engine) ğŸ›¡ï¸**
All queries pass through guardrails **before** they hit the database:

* ğŸš« **Read-only only**: blocks any DML/DDL (no `INSERT/UPDATE/DELETE/ALTER/DROP`).
* âœ… **Allowlists**: only approved tables/columns are accessible.
* ğŸ§© **Row-level filters**: tenant and permission scopes are injected automatically.
* ğŸ” **Parameterized literals**: user inputs are bound safely (no string concatenation).
* ğŸ“£ Returns a clear error when a rule blocks something (so users learn whatâ€™s allowed).

*What it looks like:*

* Blocks: `DROP TABLE invoices;` â†’ â€œMutation statements are not allowed.â€
* Enforces: automatically adds `WHERE tenant_id = :current_tenant_id` to every query.

---

**4) Explainability ğŸ•µï¸â€â™‚ï¸**
Every answer comes with:

* ğŸ“ A short **narrative** (â€œTop 10 customers by unpaid total in Q2, read-only viewâ€).
* ğŸ“Š A compact **table or chart**.
* ğŸ§¾ **Show SQL** (exact query run).
* ğŸ§¬ **Why this answer?** (high-level plan + IDs/hashes so results can be reproduced).

*What it looks like:*

* Narrative: â€œFound 127 unpaid invoices in Q2 2025; 63 customers affected. Showing top 10 by amount due.â€
* Links/buttons: **Show SQL** Â· **Why this answer?**

---

## Research context & design choices ğŸ“šğŸ”¬

Two threads shaped the design:

* **[BIRD (BI benchmark)](https://arxiv.org/pdf/2305.03111)** pushed treating NLâ†’SQL as a *real-database* problemâ€”multiple domains, wide schemas, messy namesâ€”demanding stronger **schema understanding** and **robust joins/filters** (not prompt-only shortcuts).
* **[Automatic Metadata Extraction for Text-to-SQL (Shkapenyuk et al., 2025)](https://arxiv.org/abs/2505.19988)** emphasized that *knowing whatâ€™s in the DB* narrows the NLâ†”SQL gap; LexiTau adopted â€œ**learn the database language**â€ â†’ â€œ**link questions to columns**,â€ then added production constraints: **safety** and **explanations**.

Concretely, LexiTau implements a four-stage pipeline:

1. ğŸ§­ **Learn the DB language (profiling â†’ metadata)**: build a plain-English data dictionary from column stats.
2. ğŸ”— **Schema link & draft SQL**: map NL intent to tables/columns; self-check joins/filters/literals against real data.
3. ğŸ›¡ï¸ **Policy-guarded, read-only execution**: allowlists, optional row-filters, parameterized literals; clear error if a rule blocks.
4. ğŸ” **Explainability**: short narrative + compact table/chart; â€œShow SQL / Why this answer?â€ lineage.

This evolved from a pure OSS prototype to a **hybrid** approach (Azure Form Recognizer + Azure OpenAI) for reliability while preserving the research spiritâ€”tied together with **safety**, **explainability**, and **tenant isolation**. âš™ï¸

### Where the papers show up in the codebase ğŸ—‚ï¸

**A. â€œLearn the database languageâ€ â†’ Profiling + metadata + embeddings ğŸ§­**

* Schema profiling script creates English descriptions + embeddings and upserts profiles used by the planner.
* Server-side models hold `english_description`, summaries, and `vector_embedding`â€”the **metadata surface** the planner consumes.

**B. â€œLink questions to columnsâ€ â†’ Orchestration + value/column similarity ğŸ”—**

* Linking flow selects candidate tables/columns from profiled metadata before planning SQL (paper Step 2).

**C. Safety & multitenancy (additions beyond the paper) ğŸ›¡ï¸ğŸ¢**

* Read-only policy; tenant predicate injection into FROM/JOIN/WHERE; DML/DDL blocking; **fallback binds**â€”all covered by unit tests.

**D. Explainability (addition beyond the paper) ğŸ”**

* Frontend exposes **narratives + tables/charts** and **Show SQL / Why this answer?** UI.

**E. OCR ingestion (real BI workloads) ğŸ§¾**

* Azure OCR adapters + document tasks normalize into tenant-scoped tables that NLâ†’SQL queries.

---

## Technology stack ğŸ”§

* **Backend**: FastAPI, PostgreSQL (+pgvector), SQLAlchemy, Celery/Redis, JWT auth.
* **Frontend**: Next.js/React (App Router) with chat, documents, dashboard routes and shadcn/ui components.
* **Dev/Infra**: Docker Compose, Alembic migrations.

---

## Repo map ğŸ—ºï¸

```
backend/
  alembic/versions/...       # migrations (businesses, users, documentsâ€¦)
frontend/
  src/app/(app)/
    chat/                    # chat UI (NLâ†’SQL)
    documents/               # upload + review + normalized fields/line-items
    dashboard/               # post-login summary
  components/ui              # shadcn components
  components/assistant-ui    # chat rendering
docker-compose.yml
```
